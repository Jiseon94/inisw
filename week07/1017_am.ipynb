{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "caf48fda-ed75-41e9-b592-7583e87dc88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.tokenize import regexp_tokenize\n",
    "from os import listdir\n",
    "from struct import pack, unpack\n",
    "\n",
    "def ngram(s, n=2):\n",
    "    rst = list()\n",
    "    for i in range(len(s)-(n-1)):\n",
    "        rst.append(''.join(s[i:i+n]))\n",
    "    return rst\n",
    "\n",
    "def fileids(path, ext='txt'):\n",
    "    files = list(filter(lambda f:re.search(f'{ext}$', f),\n",
    "                       listdir(path)))\n",
    "    return list(map(lambda f:f'{path}/{f}', files))\n",
    "\n",
    "p1 = re.compile(r'\\s+')\n",
    "\n",
    "D = list() # {file:문서1, maxfreq:뭐고}, 문서, ...\n",
    "V = list()\n",
    "TDM = dict() # {term1:[위치, df]}\n",
    "\n",
    "Posting = open('posting.dat', 'wb')\n",
    "Posting.close()\n",
    "\n",
    "for file in fileids('naver'):\n",
    "    i = len(D)\n",
    "    D.append({'filename':file, 'maxfreq':0, 'length':0.0})\n",
    "    \n",
    "    with open(file, 'r', encoding='utf8') as fp:\n",
    "        corpus = fp.read()\n",
    "        \n",
    "    localTDM = dict()\n",
    "    localPosting = open('local.dat', 'wb')\n",
    "\n",
    "    # Tokenizing+Normalizing\n",
    "    for t in regexp_tokenize(p1.sub(' ', corpus), r'\\b\\w+\\b'):\n",
    "        for g in ngram(t):\n",
    "            # 문서 1개 작업하는 중에, 단어가 처음 등장\n",
    "            if g not in localTDM.keys():\n",
    "                pos = localPosting.tell()\n",
    "                localPosting.write(pack('ii', 1, -1))\n",
    "                localTDM[g] = pos\n",
    "            else:\n",
    "                pos = localPosting.tell()\n",
    "                localPosting.write(pack('ii', 1, localTDM[g]))\n",
    "                localTDM[g] = pos\n",
    "                \n",
    "    localPosting.close()\n",
    "                \n",
    "    # Update; Local -> Global\n",
    "    Posting = open('posting.dat', 'ab')\n",
    "    localPosting = open('local.dat', 'rb')\n",
    "    \n",
    "    # k=단어, v=파일위치\n",
    "    maxFreq = 0\n",
    "    for k, v in localTDM.items():\n",
    "        if k not in V:\n",
    "            V.append(k)\n",
    "            \n",
    "        j = V.index(k)\n",
    "        \n",
    "        freq = 0\n",
    "        pos = v\n",
    "        while pos != -1:\n",
    "            localPosting.seek(pos)\n",
    "            f, npos = unpack('ii', localPosting.read(8))\n",
    "            pos = npos\n",
    "            freq += 1\n",
    "            \n",
    "        if freq > maxFreq:\n",
    "            maxFreq = freq\n",
    "                \n",
    "        if j not in TDM.keys():\n",
    "            pos = Posting.tell()\n",
    "            Posting.write(pack('iii', i, freq, -1))\n",
    "            TDM[j] = {'fp':pos, 'df':1}\n",
    "        else:\n",
    "            pos = Posting.tell()\n",
    "            Posting.write(pack('iii', i, freq, TDM[j]['fp']))\n",
    "            TDM[j]['fp'] = pos\n",
    "            TDM[j]['df'] += 1\n",
    "            \n",
    "    D[i]['maxfreq'] = maxFreq\n",
    "\n",
    "    localPosting.close()\n",
    "    Posting.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2b5dd5af-b693-4e63-bbf8-e2c09de86d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "i2w = lambda i:V[i]\n",
    "w2i = lambda w:V.index(w)\n",
    "\n",
    "i2d = lambda i:D[i]\n",
    "d2i = lambda d:D.index(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6b8c9050-7a32-4d49-bdb3-83d90f1ea6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log\n",
    "\n",
    "tf1 = lambda tf:1 if tf > 0 else 0\n",
    "tf2 = lambda tf:tf\n",
    "tf3 = lambda tf, ttf:tf/ttf\n",
    "tf4 = lambda tf:log(1+tf)\n",
    "tf5 = lambda tf, mtf, K=0.5:K+(1-K)*(tf/mtf)\n",
    "\n",
    "idf1 = lambda df:1\n",
    "idf2 = lambda df, N:log(N/df)\n",
    "idf3 = lambda df, N:log(N/(1+df))+1\n",
    "idf4 = lambda df, mdf:log(mdf/(1+df))\n",
    "idf5 = lambda df, N:log((N-df)/df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "06611d17-7c88-4a9d-8bdc-93a3f2ff8013",
   "metadata": {},
   "outputs": [],
   "source": [
    "WDM = dict()\n",
    "Posting = open('posting.dat', 'rb')\n",
    "Weighting = open('weighting.dat', 'wb')\n",
    "\n",
    "for k, v in TDM.items():\n",
    "    fp = Weighting.tell()\n",
    "    WDM[k] = {'fp':fp, 'df':v['df']}\n",
    "    \n",
    "    pos = v['fp']\n",
    "    while pos != -1:\n",
    "        Posting.seek(pos)\n",
    "        i, freq, npos = unpack('iii', Posting.read(12))\n",
    "        \n",
    "        tf = tf5(freq, D[i]['maxfreq'], 0)\n",
    "        idf = idf2(v['df'], len(D))\n",
    "        w = tf*idf\n",
    "        D[i]['length'] += w**2\n",
    "        Weighting.write(pack('if', i, w))\n",
    "        \n",
    "        pos = npos\n",
    "            \n",
    "Posting.close()\n",
    "Weighting.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5eb315b0-2f3f-44f5-a325-d37ef545a91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '전손차량 중 확인되지 않는 차량이 4만여대에 달한다는 지적이 나왔다. 전손차량은 자동차가 완전히 파손됐거나 침수 등으로 수리할 수 없는 상태인 자동차와 발생한 손해액이 보험가액 이상인 자동차를 말한다.'\n",
    "\n",
    "# Query Weighting\n",
    "QTM = dict()\n",
    "\n",
    "for q in regexp_tokenize(query, r'\\b\\w+\\b'):\n",
    "    for g in ngram(q): # ngram => tokenizing\n",
    "        if g in V:\n",
    "            j = w2i(g)\n",
    "            if j not in QTM.keys():\n",
    "                QTM[j] = 0\n",
    "            QTM[j] += 1\n",
    "            \n",
    "for k,v in QTM.items():\n",
    "    tf = tf5(v, max(QTM.values()), 0)\n",
    "    idf = idf2(TDM[k]['df'], len(D))\n",
    "    QTM[k] = tf*idf\n",
    "    \n",
    "# Distance\n",
    "result = dict()\n",
    "Weighting = open('weighting.dat', 'rb')\n",
    "\n",
    "for t in V:\n",
    "    j = w2i(t)\n",
    "    \n",
    "    qw = QTM.get(j, 0)\n",
    "    n = 0\n",
    "    while n < WDM[j]['df']:\n",
    "        Weighting.seek(WDM[j]['fp']+n*8)\n",
    "        i, dw = unpack('if', Weighting.read(8))\n",
    "        pos = npos\n",
    "        n += 1\n",
    "        \n",
    "        if i not in result.keys():\n",
    "            result[i] = {'dist':0.0, 'cnt':0}\n",
    "            \n",
    "        result[i]['cnt'] += 1\n",
    "        result[i]['dist'] += (qw-dw)**2\n",
    "    \n",
    "Weighting.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4d80ecfe-dc4e-456f-a1f9-078c5f40d8cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(150, {'dist': 12.477792076118877, 'cnt': 1021}),\n",
       " (85, {'dist': 13.602877702605353, 'cnt': 628}),\n",
       " (84, {'dist': 17.626246390390296, 'cnt': 622}),\n",
       " (88, {'dist': 20.463704604943832, 'cnt': 604}),\n",
       " (206, {'dist': 21.47970325384423, 'cnt': 1481}),\n",
       " (348, {'dist': 22.132515351856252, 'cnt': 353}),\n",
       " (83, {'dist': 22.703285215299353, 'cnt': 641}),\n",
       " (45, {'dist': 27.62123876701429, 'cnt': 883}),\n",
       " (243, {'dist': 28.161195300115445, 'cnt': 657}),\n",
       " (308, {'dist': 28.784139539195497, 'cnt': 162})]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(result.items(), key=lambda r:r[1]['dist'])[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "89d58396-5568-42f8-9e83-a43b29d4d78e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(54, {'dist': 350.4286582619124, 'cnt': 256}),\n",
       " (321, {'dist': 340.58596350730437, 'cnt': 243}),\n",
       " (360, {'dist': 332.23415792698154, 'cnt': 95}),\n",
       " (356, {'dist': 306.40029529272033, 'cnt': 633}),\n",
       " (52, {'dist': 281.07616406652653, 'cnt': 264}),\n",
       " (165, {'dist': 267.8058227291391, 'cnt': 219}),\n",
       " (211, {'dist': 256.97845924989787, 'cnt': 411}),\n",
       " (67, {'dist': 252.79960945506326, 'cnt': 431}),\n",
       " (163, {'dist': 249.40806888739272, 'cnt': 199}),\n",
       " (96, {'dist': 245.6643303010328, 'cnt': 523})]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(result.items(), key=lambda r:r[1]['dist'], reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "13ad9dd9-ff70-4717-9abe-add0f005285b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# query = '전손차량 중 확인되지 않는 차량이 4만여대에 달한다는 지적이 나왔다. 전손차량은 자동차가 완전히 파손됐거나 침수 등으로 수리할 수 없는 상태인 자동차와 발생한 손해액이 보험가액 이상인 자동차를 말한다.'\n",
    "with open(D[22]['filename'], 'r', encoding='utf8') as f:\n",
    "    query = p1.sub(' ', f.read())\n",
    "\n",
    "# Query Weighting\n",
    "QTM = dict()\n",
    "\n",
    "for q in regexp_tokenize(query, r'\\b\\w+\\b'):\n",
    "    for g in ngram(q): # ngram => tokenizing\n",
    "        if g in V:\n",
    "            j = w2i(g)\n",
    "            if j not in QTM.keys():\n",
    "                QTM[j] = 0\n",
    "            QTM[j] += 1\n",
    "            \n",
    "for k,v in QTM.items():\n",
    "    tf = tf5(v, max(QTM.values()), 0)\n",
    "    idf = idf2(TDM[k]['df'], len(D))\n",
    "    QTM[k] = tf*idf\n",
    "\n",
    "ql = sum([v**2 for k, v in QTM.items()])**(1/2)\n",
    "for k,v in QTM.items():\n",
    "    QTM[k] = v\n",
    "    \n",
    "# Angle\n",
    "result = dict()\n",
    "Weighting = open('weighting.dat', 'rb')\n",
    "\n",
    "for k, v in QTM.items():\n",
    "    j = k\n",
    "    \n",
    "    n = 0\n",
    "    while n < WDM[j]['df']:\n",
    "        Weighting.seek(WDM[j]['fp']+n*8)\n",
    "        i, dw = unpack('if', Weighting.read(8))\n",
    "        pos = npos\n",
    "        n += 1\n",
    "        \n",
    "        if i not in result.keys():\n",
    "            result[i] = {'dist':0.0, 'cnt':0}\n",
    "            \n",
    "        result[i]['cnt'] += 1\n",
    "        result[i]['dist'] += v*dw # 내적\n",
    "\n",
    "for k, v in result.items(): # 정규화\n",
    "    result[k]['dist'] /= ql * D[k]['length']**(1/2)\n",
    "    #                    쿼리길이      다큐먼트길이\n",
    "    \n",
    "Weighting.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b24bf003-f9e6-41f8-96b5-628027738165",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(22, {'dist': 0.9397504593905724, 'cnt': 168}),\n",
       " (89, {'dist': 0.4060440391097903, 'cnt': 79}),\n",
       " (76, {'dist': 0.36888435213755366, 'cnt': 79}),\n",
       " (37, {'dist': 0.3641186046231235, 'cnt': 61}),\n",
       " (315, {'dist': 0.25947112604165945, 'cnt': 62}),\n",
       " (59, {'dist': 0.2523920457005235, 'cnt': 61}),\n",
       " (232, {'dist': 0.19459779266653335, 'cnt': 62}),\n",
       " (34, {'dist': 0.13987475721531556, 'cnt': 38}),\n",
       " (60, {'dist': 0.1372369108149422, 'cnt': 32}),\n",
       " (85, {'dist': 0.09548805435195587, 'cnt': 25})]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(result.items(), key=lambda r:r[1]['dist'], reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "184be131-ac06-4b45-a9cf-2eec6a1c0f4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(125, {'dist': 0.0001651989406841924, 'cnt': 1}),\n",
       " (177, {'dist': 0.00039167303195381805, 'cnt': 5}),\n",
       " (331, {'dist': 0.0006214405081663085, 'cnt': 3}),\n",
       " (323, {'dist': 0.0008902586480422132, 'cnt': 4}),\n",
       " (116, {'dist': 0.0008999992092287987, 'cnt': 4}),\n",
       " (378, {'dist': 0.001146442570330567, 'cnt': 9}),\n",
       " (250, {'dist': 0.0012972282319964764, 'cnt': 9}),\n",
       " (290, {'dist': 0.001428416328520135, 'cnt': 7}),\n",
       " (229, {'dist': 0.0014584330771862323, 'cnt': 8}),\n",
       " (314, {'dist': 0.0015138025755681585, 'cnt': 8})]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(result.items(), key=lambda r:r[1]['dist'])[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "fe7a55e5-d393-42e5-8249-08117dfe9627",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " ◀앵커▶장수군의 관광 명소로 떠오르고 있는 누리파크에 이색적인 분위기의 감성 정원이 조성됐습니다.완주 운주면에 아홉 번째 작은 도서관이 문을 열었습니다.우리 지역 소식 허현호 기자입니다.◀리포트▶[장수]장수군이 누리파크 일원에 유럽형 가족정원을 조성해 관람객들의 발길이 이어지고 있습니다.'한국의 타샤 튜더'로 불리는 임지수 정원가드너가 총괄한 가족정원은 연꽃정원과 맨발정원, 물빛정원 등을 테마로 희귀한 꽃과 이국적인 나무가 심어져 있습니다.또 가을을 맞아 국화꽃길과 웨딩정원도 마련돼 색다른 볼거리를 제공하고 있습니다.[최석원 / 장수군 산림공원과 산림정책팀장]\"누리파크를 아이들과 어른들이 함께할 수 있는 이색적인 가족정원으로 만들어 대한민국을 대표하는 관광지로 만들어나가겠습니다.\"[완주]완주 운주면행정복지센터에 '구름골 작은도서관'이 개관했습니다.운주면민들의 문화사랑방이 될 도서관은 2천200여권의 장서가 비치됐고, 소모임 활동과 책 읽기, 독서 프로그램이 가능한 공간으로 꾸며졌습니다.완주군은 누구나 도서관 문화를 누릴 수 있도록 작은도서관을 조성해 현재 8개 읍·면에서 운영하고 있습니다.[김미경 / 완주군도서관사업소 도서관정책팀장]\"작은도서관이 우리 아이들의 꿈과 희망이 넘치는 곳이고, 어르신들이 정을 나누는 행복한 공간이 됐으면 좋겠습니다.\"[남원]전라북도에서 추진하는 농공단지 환경개선 공모사업에 남원시가 선정돼 총사업비 2억 원을 확보했습니다.남원시는 노암농공단지 구내식당 환경개선과 함께 근로자 편의 공간과 휴게실을 확충하는 등 일하기 좋은 환경을 조성해 지역 청년과 노동자들을 유입시킬 계획입니다.[익산]익산시가 세계문화유산과 야간경관 콘텐츠 등 지역 관광자원을 활용해 국외 관광객 유치에 나섭니다.익산시는 중국과 대만, 홍콩 등 중화권을 전담하는 여행사와 공동마케팅 업무협약을 맺고, 특수목적 관광객과 수학여행단 등 외래 관광객 유치를 위한 관광상품을 개발하고 홍보할 방침입니다.MBC뉴스 허현호입니다.영상편집:김관중영상제공:장수군(최민광), 완주군(김회성), 남원시(강석현), 익산시(최성규)\n",
      "\t\t\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(D[0]['filename'], 'r', encoding='utf8') as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "fb5b6108-971e-41ee-9044-dbe8b27a407a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 앵커 장수군의 관광 명소로 떠오르고 있는 누리파크에 이색적인 분위기의 감성 정원이 조성됐습니다.완주 운주면에 아홉 번째 작은 도서관이 문을 열었습니다.우리 지역 소식 허현호 기자입니다. 리포트 장수 장수군이 누리파크 일원에 유럽형 가족정원을 조성해 관람객들의 발길이 이어지고 있습니다. 한국의 타샤 튜더 로 불리는 임지수 정원가드너가 총괄한 가족정원은 연꽃정원과 맨발정원, 물빛정원 등을 테마로 희귀한 꽃과 이국적인 나무가 심어져 있습니다.또 가을을 맞아 국화꽃길과 웨딩정원도 마련돼 색다른 볼거리를 제공하고 있습니다. 최석원 장수군 산림공원과 산림정책팀장 누리파크를 아이들과 어른들이 함께할 수 있는 이색적인 가족정원으로 만들어 대한민국을 대표하는 관광지로 만들어나가겠습니다. 완주 완주 운주면행정복지센터에 구름골 작은도서관 이 개관했습니다.운주면민들의 문화사랑방이 될 도서관은 2천200여권의 장서가 비치됐고, 소모임 활동과 책 읽기, 독서 프로그램이 가능한 공간으로 꾸며졌습니다.완주군은 누구나 도서관 문화를 누릴 수 있도록 작은도서관을 조성해 현재 8개 읍 면에서 운영하고 있습니다. 김미경 완주군도서관사업소 도서관정책팀장 작은도서관이 우리 아이들의 꿈과 희망이 넘치는 곳이고, 어르신들이 정을 나누는 행복한 공간이 됐으면 좋겠습니다. 남원 전라북도에서 추진하는 농공단지 환경개선 공모사업에 남원시가 선정돼 총사업비 2억 원을 확보했습니다.남원시는 노암농공단지 구내식당 환경개선과 함께 근로자 편의 공간과 휴게실을 확충하는 등 일하기 좋은 환경을 조성해 지역 청년과 노동자들을 유입시킬 계획입니다. 익산 익산시가 세계문화유산과 야간경관 콘텐츠 등 지역 관광자원을 활용해 국외 관광객 유치에 나섭니다.익산시는 중국과 대만, 홍콩 등 중화권을 전담하는 여행사와 공동마케팅 업무협약을 맺고, 특수목적 관광객과 수학여행단 등 외래 관광객 유치를 위한 관광상품을 개발하고 홍보할 방침입니다.MBC뉴스 허현호입니다.영상편집 김관중영상제공 장수군 최민광 , 완주군 김회성 , 남원시 강석현 , 익산시 최성규 \n"
     ]
    }
   ],
   "source": [
    "#전처리 기를 하나 만들어서 걸어야 할 표현 거르기\n",
    "p1 = re.compile(r'\\s+')  #위아래 공백 없애기\n",
    "p2 = re.compile(r'^\\s+')\n",
    "p3 = re.compile(r'\\s+$')\n",
    "p4 = re.compile(r'[^0-9a-zA-Zㄱ-ㅎㅏ-ㅣ가-힣.,% ]')  #구두점 날리기\n",
    "\n",
    "def preprocessing(file):\n",
    "    with open(file, 'r', encoding='utf8') as fp:\n",
    "        doc = fp.read()\n",
    "        print(p3.sub(' ', (p2.sub(' ', (p1.sub(' ', p4.sub(' ', doc)))))))\n",
    "        #순서에 따라 결과 달라짐. \n",
    "\n",
    "preprocessing(D[0]['filename'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "9165ab06-8ede-415b-ad38-c553e5c30ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#위 아래 공백이 사라진걸 볼 수 있다. 쓸데없는 기호도 사라진게 보인다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "1f64be71-2869-485b-adcb-5fcf3cd556c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#내 데이터에 따라서 전처리 patterns 만들어주고 적용하기\n",
    "\n",
    "p1 = re.compile(r'\\s+')  #위아래 공백 없애기\n",
    "p2 = re.compile(r'^\\s+')\n",
    "p3 = re.compile(r'\\s+$')\n",
    "p4 = re.compile(r'[^0-9a-zA-Zㄱ-ㅎㅏ-ㅣ가-힣.,% ]')  #구두점 날리기\n",
    "\n",
    "def preprocessing(file):\n",
    "    with open(file, 'r', encoding='utf8') as fp:\n",
    "        doc = p3.sub(' ',\n",
    "              p2.sub(' ',\n",
    "              p1.sub(' ',\n",
    "              p4.sub(' ', fp.read()))))\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "42aaa1d5-b42a-48f6-920a-c1628d23207d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from konlpy.tag import Okt\n",
    "\n",
    "ma = Okt()\n",
    "\n",
    "def tokenizing(doc):\n",
    "    term = list()\n",
    "    \n",
    "    for s in sent_tokenize(doc):\n",
    "        s1= word_tokenize(s)\n",
    "        s2= ma.morphs(s)\n",
    "\n",
    "\n",
    "        #2음절씩 보기\n",
    "        for g in ngram(s):\n",
    "            term.append(g)\n",
    "            \n",
    "        for t in s1:\n",
    "            term.append(t)\n",
    "\n",
    "        for g in ngram(s1):\n",
    "            term.append(g)\n",
    "            \n",
    "        #형태소 분석한것도 쓰기\n",
    "        for t in s2:\n",
    "            term.append(t)\n",
    "\n",
    "        for g in ngram(s2):\n",
    "            term.append(g)\n",
    "\n",
    "    return term\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "5ee5e423-971b-4553-9a6f-5da958d6265b",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'filename'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[99], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m tokenizing(preprocessing(\u001b[43mD\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfilename\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m))\n",
      "\u001b[1;31mKeyError\u001b[0m: 'filename'"
     ]
    }
   ],
   "source": [
    "tokenizing(preprocessing(D[0]['filename']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "b54048f7-bbf6-436f-9c91-c52b78c69644",
   "metadata": {},
   "outputs": [],
   "source": [
    "#색인기 만들기\n",
    "from collections import Counter\n",
    "\n",
    "def indexer(tokens):\n",
    "    localTDM = Counter(tokens)\n",
    "    #Counter 로 처리하면 키 밸류 쌍으로 처리가 된다.\n",
    "    return localTDM, max(localTDM.values()) #나중에 정규화를 위해 이 값들이 필요한거다.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "597d65b9-dbaf-4ba0-bc1b-5430a75e5f11",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[105], line 30\u001b[0m\n\u001b[0;32m     27\u001b[0m     TDM[j] \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpos1\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpos2\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdf\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;241m0\u001b[39m}\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m#file 에다가 기록하기\u001b[39;00m\n\u001b[1;32m---> 30\u001b[0m pos \u001b[38;5;241m=\u001b[39m \u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtell\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m#현재 위치 알려줘\u001b[39;00m\n\u001b[0;32m     31\u001b[0m fp\u001b[38;5;241m.\u001b[39mwrite(pack(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124miii\u001b[39m\u001b[38;5;124m'\u001b[39m, i, v, TDM[j][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpos1\u001b[39m\u001b[38;5;124m'\u001b[39m]))  \u001b[38;5;66;03m#어떤문서에서 몇번 나왔고 마지막 위치 (pos : -1)\u001b[39;00m\n\u001b[0;32m     32\u001b[0m TDM[j][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpos1\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m pos\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "D = list()\n",
    "V = list()\n",
    "TDM = dict()\n",
    "\n",
    "#Posting file 관련된 것들\n",
    "posting = 'posting.dat'\n",
    "\n",
    "#훼이크로 빈파일 생성하고 닫기\n",
    "open(posting, 'wb').close()\n",
    "\n",
    "for file in fileids('naver')[:100]:\n",
    "    i = len(D)\n",
    "    localTDM, maxFreq = indexer(tokenizing(preprocessing(file)))\n",
    "    D.append({'file':file, 'mf': maxFreq, 'dl': 0.0})\n",
    "\n",
    "#원래는 여기서 merge 를 해줘야함.\n",
    "    #merge 부분에서 posting 열기\n",
    "    fp = open(posting, 'ab')\n",
    "    #merge 에서는 localTDM 을 키 밸류로 돌릴거다.\n",
    "    for k, v in localTDM.items():\n",
    "        if k not in V:\n",
    "            V.append(k)\n",
    "        j = w2i(k)\n",
    "\n",
    "        if j not in TDM:\n",
    "            #j에 안들어있으면, 위치가 지금 어딘지, document frequency 가 어떻게 되는지\n",
    "            TDM[j] = {'pos1':-1,'pos2':-1, 'df':0}\n",
    "\n",
    "        #file 에다가 기록하기\n",
    "        pos = fp.tell() #현재 위치 알려줘\n",
    "        fp.write(pack('iii', i, v, TDM[j]['pos1']))  #어떤문서에서 몇번 나왔고 마지막 위치 (pos : -1)\n",
    "        TDM[j]['pos1'] = pos\n",
    "        TDM[j]['df'] += 1\n",
    "    fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "3ef90fa1-8af5-48cb-ad37-8d2ae9d0e8dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#위 코드 전체가 색인하는 과정이다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f25c95-27bf-4b79-ab03-b1a610b3f3c9",
   "metadata": {},
   "source": [
    "### weight 주기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "22ab0d18-0dff-44da-84e5-814a30b311f8",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[103], line 18\u001b[0m\n\u001b[0;32m     15\u001b[0m pos1 \u001b[38;5;241m=\u001b[39m npos1\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m#weight 주기\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m w \u001b[38;5;241m=\u001b[39m tf5(freq, \u001b[43mD\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmf\u001b[39m\u001b[38;5;124m'\u001b[39m],\u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m*\u001b[39m idf2(v[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdf\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;28mlen\u001b[39m(D))\n\u001b[0;32m     19\u001b[0m fp2\u001b[38;5;241m.\u001b[39mwrite(pack(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mif\u001b[39m\u001b[38;5;124m'\u001b[39m, i, w))\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m#D에 기록하기\u001b[39;00m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "weighting = 'weighting.dat'\n",
    "\n",
    "#똑같이 TDM 열기\n",
    "fp1 = open(posting, 'rb')\n",
    "fp2 = open(weighting, 'wb')\n",
    "\n",
    "for k, v in TDM.items():\n",
    "    j = k\n",
    "\n",
    "    pos1 = v['pos1']\n",
    "    pos2 = fp2.tell()\n",
    "    while pos1 != -1:\n",
    "        fp1.seek(pos1)\n",
    "        i, freq, npos1 = unpack('iii', fp1.read(12))\n",
    "        pos1 = npos1\n",
    "\n",
    "        #weight 주기\n",
    "        w = tf5(freq, D[i]['mf'],0) * idf2(v['df'], len(D))\n",
    "        fp2.write(pack('if', i, w))\n",
    "\n",
    "        #D에 기록하기\n",
    "        D[i]['dl'] += w**2\n",
    "    #weight 기록하기\n",
    "    TDM[j]['pos2'] = pos2\n",
    "    \n",
    "fp1.close()\n",
    "fp2.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9cc249e-e201-48d0-86bf-dda34683aac7",
   "metadata": {},
   "source": [
    "### query 를 가지고 해보기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "63333254-860f-4892-bebb-801402c0e26b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "'중위' is not in list",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[104], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m QTM \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m()\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m tokens\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m----> 7\u001b[0m     j \u001b[38;5;241m=\u001b[39m \u001b[43mw2i\u001b[49m\u001b[43m(\u001b[49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m     w \u001b[38;5;241m=\u001b[39m tf5(v, maxFreq, \u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m*\u001b[39m idf2(TDM[j][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdf\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;28mlen\u001b[39m(D))\n\u001b[0;32m      9\u001b[0m     QTM[j] \u001b[38;5;241m=\u001b[39m w \n",
      "Cell \u001b[1;32mIn[45], line 2\u001b[0m, in \u001b[0;36m<lambda>\u001b[1;34m(w)\u001b[0m\n\u001b[0;32m      1\u001b[0m i2w \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m i:V[i]\n\u001b[1;32m----> 2\u001b[0m w2i \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m w:\u001b[43mV\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m(\u001b[49m\u001b[43mw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m i2d \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m i:D[i]\n\u001b[0;32m      5\u001b[0m d2i \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m d:D\u001b[38;5;241m.\u001b[39mindex(d)\n",
      "\u001b[1;31mValueError\u001b[0m: '중위' is not in list"
     ]
    }
   ],
   "source": [
    "Q = fileids('naver')[10]\n",
    "\n",
    "tokens, maxFreq = indexer(tokenizing(preprocessing(Q)))\n",
    "\n",
    "QTM = dict()\n",
    "for k, v in tokens.items():\n",
    "    j = w2i(k)\n",
    "    w = tf5(v, maxFreq, 0) * idf2(TDM[j]['df'], len(D))\n",
    "    QTM[j] = w \n",
    "\n",
    "q1 = sum([w**2 for w in QTM.values()])**.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5ce909-2180-451f-9fcc-19e24e702530",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = dict()\n",
    "\n",
    "fp = open(weighting, 'rb')\n",
    "\n",
    "for j, qw in QTM.items():\n",
    "    n = 0\n",
    "    pos = TDM[j]['pos2']\n",
    "    while n < TDM[j]['df']:\n",
    "        fp.seek(pos+n*8)\n",
    "        i, tw = unpack('if', fp.read(8))\n",
    "        n +=1\n",
    "\n",
    "        if i not in result:\n",
    "            result[i] = .0\n",
    "            \n",
    "        result[i] += qw * tw\n",
    "\n",
    "for k, v in result.items():\n",
    "    # result[k] = v / (ql * (D[k]['dl']**.5))\n",
    "    result[k] = v / (D[k]['dl']**.5)\n",
    "\n",
    "fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89741e2-9eab-4c86-a692-36c01f81dd7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(result.items(), key=lambda r:r[1], reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c762ae3-3b93-408d-a9d6-03c5355332d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, sim in sorted(result.items(),\n",
    "                     key=lambda r:r[1], reverse=True)[:10]\n",
    "    print(D[i]['file'], sim)\n",
    "    print(preprocessing(D[i]['file'])[:100])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aadd9732-8b4c-4238-a0a4-93cbd95685e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#여기까지가 우리가 아는 검색 방법이라고 한다. sorting 에 따라서 구글이 되기도 네이버가 되기도 함. \n",
    "#저러한 색인어들이 꼭 필요한건 아니다. \n",
    "#classification, clustering - 적은 컬럼이 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368529e0-7ed8-45a0-ab45-34a2ad86018b",
   "metadata": {},
   "source": [
    "### 저장하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a5659b-2697-4448-ac69-e0a3750e799d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('D.dat', 'w', encoding='utf8') as fp:\n",
    "    json.dump(D, fp)\n",
    "\n",
    "with open('V.dat', 'w', encoding='utf8') as fp:\n",
    "    json.dump(V, fp)\n",
    "\n",
    "with open('TDM.dat', 'w', encoding='utf8') as fp:\n",
    "    json.dump(TDM, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40985ea2-2474-4f33-b564-8af39490df47",
   "metadata": {},
   "outputs": [],
   "source": [
    "!dir *.dat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc7708a-95e8-4ef0-8b0b-a11534f27db3",
   "metadata": {},
   "source": [
    "## 수집기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9072408-6321-402d-b761-a1e323d35780",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 네이버 뉴스 수집기 - Crawler\n",
    "from requests import get\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "urls = list()\n",
    "seens = list()\n",
    "\n",
    "path = 'naver/'\n",
    "urls.append('https://news.naver.com/')\n",
    "\n",
    "headers = {'user-agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/117.0.0.0 Safari/537.36'}\n",
    "\n",
    "while urls:\n",
    "    url = urls.pop(0)\n",
    "    seens.append(url)\n",
    "    \n",
    "    dom = BeautifulSoup(get(url,headers=headers).text, 'html.parser')\n",
    "    \n",
    "    # 메뉴\n",
    "    alist = dom.select('ul[role=menu] > li > a[href]')\n",
    "    for a in alist:\n",
    "        if re.search(r'sid1=\\d{3}$', a.attrs['href']):\n",
    "            if a.attrs['href'] not in urls and\\\n",
    "               a.attrs['href'] not in seens:\n",
    "                urls.append(a.attrs['href'])\n",
    "                \n",
    "    # 기사링크 => 정교하게\n",
    "    alist = dom.select('''\n",
    "                        .sh_text > a[href],\n",
    "                        .cluster_text > a[href],\n",
    "                        dt > a[href]\n",
    "                    ''')\n",
    "    for a in alist:\n",
    "        if a.attrs['href'] not in urls and\\\n",
    "           a.attrs['href'] not in seens:\n",
    "            urls.append(a.attrs['href'])\n",
    "            \n",
    "    # 본문\n",
    "    news = dom.select_one('#newsct_article')\n",
    "    if news:\n",
    "        # https://n.news.naver.com/article/005/0001644886?cds=news_media_pc\n",
    "        file = '-'.join(\n",
    "            re.search(r'(\\d{10})\\?sid=(\\d{3})', url).groups())\n",
    "        with open(f'{path}{file}.txt', 'w', encoding='utf-8') as fp:\n",
    "            fp.write(news.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e40e94-5bae-4c84-9435-2861f8ffd157",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b5550e-8e3a-418e-83fe-76ab90b353cf",
   "metadata": {},
   "source": [
    "## load 하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a087830e-a4b2-4e42-912f-e06655154536",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('D.dat', encoding='utf8') as fp:\n",
    "    D = json.load(fp)\n",
    "\n",
    "with open('V.dat', encoding='utf8') as fp:\n",
    "    V = json.load(fp)\n",
    "\n",
    "with open('TDM.dat', encoding='utf8') as fp:\n",
    "    TDM = json.load(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "962233ad-480e-4ea6-be33-5eda4c59c7f6",
   "metadata": {},
   "source": [
    "### dynamic 으로 다시 검색을 해서, KNN 이용..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377451f6-5975-4710-9ee4-59b59f300911",
   "metadata": {},
   "outputs": [],
   "source": [
    "p1 = re.compile(r'\\s+')  #위아래 공백 없애기\n",
    "p2 = re.compile(r'^\\s+')\n",
    "p3 = re.compile(r'\\s+$')\n",
    "p4 = re.compile(r'[^0-9a-zA-Zㄱ-ㅎㅏ-ㅣ가-힣.,% ]')  #구두점 날리기\n",
    "\n",
    "def preprocessing(file):\n",
    "    with open(file, 'r', encoding='utf8') as fp:\n",
    "        doc = p3.sub(' ',\n",
    "              p2.sub(' ',\n",
    "              p1.sub(' ',\n",
    "              p4.sub(' ', fp.read()))))\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f105bdc3-a7d0-41a0-8204-34849ac23d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#색인기 만들기\n",
    "from collections import Counter\n",
    "\n",
    "def indexer(tokens):\n",
    "    localTDM = Counter(tokens)\n",
    "    #Counter 로 처리하면 키 밸류 쌍으로 처리가 된다.\n",
    "    return localTDM, max(localTDM.values()) #나중에 정규화를 위해 이 값들이 필요한거다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "0dc407dc-dedf-4d64-9d64-0d62fbb4290f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(77, 48844)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(D), len(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "8f5eb0fb-878f-497a-b1af-5eceda0febab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(77, 48844)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(D), len(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "9deadcb1-cf9b-4278-8ec3-d67a4907ae24",
   "metadata": {},
   "outputs": [],
   "source": [
    "posting = 'posting.dat'\n",
    "\n",
    "for file in fileids('naver')[:10]: #시간 오래 걸려서 [:10] 으로 함. ...\n",
    "    if len(list(filter(lambda r:r['file'] == file, D))) >0:\n",
    "        continue\n",
    "        \n",
    "    i = len(D)\n",
    "    localTDM, maxFreq = indexer(tokenizing(preprocessing(file)))\n",
    "    D.append({'file':file, 'mf': maxFreq, 'dl': 0.0})\n",
    "\n",
    "    fp = open(posting, 'ab')\n",
    "    for k, v in localTDM.items():\n",
    "        if k not in V:\n",
    "            V.append(k)\n",
    "        j = w2i(k)\n",
    "\n",
    "        if j not in TDM:\n",
    "            TDM[j] = {'pos1':-1,'pos2':-1, 'df':0}\n",
    "\n",
    "        pos = fp.tell() #현재 위치 알려줘\n",
    "        fp.write(pack('iii', i, v, TDM[j]['pos1']))  #어떤문서에서 몇번 나왔고 마지막 위치 (pos : -1)\n",
    "        TDM[j]['pos1'] = pos\n",
    "        TDM[j]['df'] += 1\n",
    "    fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80efa4c-33ce-4a84-b106-4d2cf3fe089d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.tokenize import regexp_tokenize\n",
    "from os import listdir\n",
    "from struct import pack, unpack\n",
    "\n",
    "def ngram(s, n=2):\n",
    "    rst = list()\n",
    "    for i in range(len(s)-(n-1)):\n",
    "        rst.append(''.join(s[i:i+n]))\n",
    "    return rst\n",
    "\n",
    "def fileids(path, ext='txt'):\n",
    "    files = list(filter(lambda f:re.search(f'{ext}$', f),\n",
    "                       listdir(path)))\n",
    "    return list(map(lambda f:f'{path}/{f}', files))\n",
    "\n",
    "p1 = re.compile(r'\\s+')\n",
    "\n",
    "D = list() # {file:문서1, maxfreq:뭐고}, 문서, ...\n",
    "V = list()\n",
    "TDM = dict() # {term1:[위치, df]}\n",
    "\n",
    "Posting = open('posting.dat', 'wb')\n",
    "Posting.close()\n",
    "\n",
    "for file in fileids('naver'):\n",
    "    i = len(D)\n",
    "    D.append({'filename':file, 'maxfreq':0, 'length':0.0})\n",
    "    \n",
    "    with open(file, 'r', encoding='utf8') as fp:\n",
    "        corpus = fp.read()\n",
    "        \n",
    "    localTDM = dict()\n",
    "    localPosting = open('local.dat', 'wb')\n",
    "\n",
    "    # Tokenizing+Normalizing\n",
    "    for t in regexp_tokenize(p1.sub(' ', corpus), r'\\b\\w+\\b'):\n",
    "        for g in ngram(t):\n",
    "            # 문서 1개 작업하는 중에, 단어가 처음 등장\n",
    "            if g not in localTDM.keys():\n",
    "                pos = localPosting.tell()\n",
    "                localPosting.write(pack('ii', 1, -1))\n",
    "                localTDM[g] = pos\n",
    "            else:\n",
    "                pos = localPosting.tell()\n",
    "                localPosting.write(pack('ii', 1, localTDM[g]))\n",
    "                localTDM[g] = pos\n",
    "                \n",
    "    localPosting.close()\n",
    "                \n",
    "    # Update; Local -> Global\n",
    "    Posting = open('posting.dat', 'ab')\n",
    "    localPosting = open('local.dat', 'rb')\n",
    "    \n",
    "    # k=단어, v=파일위치\n",
    "    maxFreq = 0\n",
    "    for k, v in localTDM.items():\n",
    "        if k not in V:\n",
    "            V.append(k)\n",
    "            \n",
    "        j = V.index(k)\n",
    "        \n",
    "        freq = 0\n",
    "        pos = v\n",
    "        while pos != -1:\n",
    "            localPosting.seek(pos)\n",
    "            f, npos = unpack('ii', localPosting.read(8))\n",
    "            pos = npos\n",
    "            freq += 1\n",
    "            \n",
    "        if freq > maxFreq:\n",
    "            maxFreq = freq\n",
    "                \n",
    "        if j not in TDM.keys():\n",
    "            pos = Posting.tell()\n",
    "            Posting.write(pack('iii', i, freq, -1))\n",
    "            TDM[j] = {'fp':pos, 'df':1}\n",
    "        else:\n",
    "            pos = Posting.tell()\n",
    "            Posting.write(pack('iii', i, freq, TDM[j]['fp']))\n",
    "            TDM[j]['fp'] = pos\n",
    "            TDM[j]['df'] += 1\n",
    "            \n",
    "    D[i]['maxfreq'] = maxFreq\n",
    "\n",
    "    localPosting.close()\n",
    "    Posting.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf3d717-a826-4fd6-8531-070d88975763",
   "metadata": {},
   "source": [
    "### weighting 해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "7330aeef-ebbd-4de1-8771-9ec30d22d480",
   "metadata": {},
   "outputs": [],
   "source": [
    "weighting = 'weighting.dat'\n",
    "\n",
    "#똑같이 TDM 열기\n",
    "fp1 = open(posting, 'rb')\n",
    "fp2 = open(weighting, 'wb')\n",
    "\n",
    "for k, v in TDM.items():\n",
    "    j = k\n",
    "\n",
    "    pos1 = v['pos1']\n",
    "    pos2 = fp2.tell()\n",
    "    while pos1 != -1:\n",
    "        fp1.seek(pos1)\n",
    "        i, freq, npos1 = unpack('iii', fp1.read(12))\n",
    "        pos1 = npos1\n",
    "\n",
    "        #weight 주기\n",
    "        w = tf5(freq, D[i]['mf'],0) * idf2(v['df'], len(D))\n",
    "        fp2.write(pack('if', i, w))\n",
    "\n",
    "        #D에 기록하기\n",
    "        D[i]['dl'] += w**2\n",
    "    #weight 기록하기\n",
    "    TDM[j]['pos2'] = pos2\n",
    "    \n",
    "fp1.close()\n",
    "fp2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "b91ad3a1-cc57-40f6-9784-ffb9a27d34be",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = fileids('naver')[60]\n",
    "\n",
    "tokens, maxFreq = indexer(tokenizing(preprocessing(Q)))\n",
    "\n",
    "QTM = dict()\n",
    "for k, v in tokens.items():\n",
    "    j = w2i(k)\n",
    "    w = tf5(v, maxFreq, 0) * idf2(TDM[j]['df'], len(D))\n",
    "    QTM[j] = w \n",
    "\n",
    "q1 = sum([w**2 for w in QTM.values()])**.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "732c4110-781b-433a-873b-4942e755d287",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18591, 48844)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V.index('이균'), len(TDM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742dc0d7-fb03-428d-951d-9a5a92447f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "fp = open(weighting, 'rb')\n",
    "pos = TDM[0]['pos2']\n",
    "\n",
    "n = 0\n",
    "while n < TDM[0]['df']:\n",
    "    fp.seek(pos+n*8)\n",
    "    print(unpack('if', fp.read(8)))\n",
    "fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "fd299548-0349-44ae-81f8-8a325a648317",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8410ce1a-6234-4eb6-8182-55a564952dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 10 \n",
    "\n",
    "\n",
    "for i, sim in sorted(result.items(),\n",
    "                     key=lambda r:r[1], reverse=True)[:10]\n",
    "    print(D[i]['file'], sim)\n",
    "    print(preprocessing(D[i]['file'])[:100])\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
